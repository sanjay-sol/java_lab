EXP-1

Data cleaning
•	import pandas as pd
•	import numpy as np 
•	import seaborn as sns 
•	from sklearn import datasets
•	data=pd.read_csv('iris.csv')
•	data.head()
•	data.shape 
•	data.shape 
•	data.describe()
•	data.info()
•	data.isnull().sum()
•	new_data = data.dropna()
    print(new_data.to_string())
	data.dropna(subset=['petal_width'], inplace = True)
	data.loc[7, 'sepal_length'] = 4.5 
•	for x in data.index:
  if data.loc[x, "petal_length"] >1.4:
   data.loc[x, "petal_length"] = 1.4
	print(data.duplicated())
	data.drop_duplicates(inplace = True) 
Data Reduction
•	from sklearn import datasets
     iris=datasets.load_iris()
     x=iris.data 
     y=iris.target 
    df=pd.DataFrame(x)
    print(df.head()) 
•	# IMPORT THE PANDAS LIBRARY 
•	# TO USE THE DATAFRAME TOOL 
•	import pandas as pd 
•	  
•	# IMPORT THE IRIS DATA FROM THE  
•	# SKLEARN MODULE 
•	from sklearn.datasets import load_iris 
•	  
•	# LOAD THE IRIS DATASET BY CALLING 
•	# THE FUNCTION 
•	iris_data = load_iris() 
•	  
•	# PLACE THE IRIS DATA IN A PANDAS 
•	# DATAFRAME 
•	df = pd.DataFrame(data=iris_data.data,  
•	                  columns=iris_data.feature_names) 
•	  
•	# DISPLAY FIRST 5 RECORDS OF THE  
•	# DATAFRAME 
•	df.head() 
•	corr_matrix=df.corr().abs()
    print(corr_matrix) 
	sns.heatmap(df.corr(),annot=True); 


EXP-1

Data cleaning
•	import pandas as pd
•	import numpy as np 
•	import seaborn as sns 
•	from sklearn import datasets
•	data=pd.read_csv('iris.csv')
•	data.head()
•	data.shape 
•	data.shape 
•	data.describe()
•	data.info()
•	data.isnull().sum()
•	new_data = data.dropna()
    print(new_data.to_string())
	data.dropna(subset=['petal_width'], inplace = True)
	data.loc[7, 'sepal_length'] = 4.5 
•	for x in data.index:
  if data.loc[x, "petal_length"] >1.4:
   data.loc[x, "petal_length"] = 1.4
	print(data.duplicated())
	data.drop_duplicates(inplace = True) 
Data Reduction
•	from sklearn import datasets
     iris=datasets.load_iris()
     x=iris.data 
     y=iris.target 
    df=pd.DataFrame(x)
    print(df.head()) 
•	# IMPORT THE PANDAS LIBRARY 
•	# TO USE THE DATAFRAME TOOL 
•	import pandas as pd 
•	  
•	# IMPORT THE IRIS DATA FROM THE  
•	# SKLEARN MODULE 
•	from sklearn.datasets import load_iris 
•	  
•	# LOAD THE IRIS DATASET BY CALLING 
•	# THE FUNCTION 
•	iris_data = load_iris() 
•	  
•	# PLACE THE IRIS DATA IN A PANDAS 
•	# DATAFRAME 
•	df = pd.DataFrame(data=iris_data.data,  
•	                  columns=iris_data.feature_names) 
•	  
•	# DISPLAY FIRST 5 RECORDS OF THE  
•	# DATAFRAME 
•	df.head() 
•	corr_matrix=df.corr().abs()
    print(corr_matrix) 
	sns.heatmap(df.corr(),annot=True); 


EXP-2



import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix,accuracy_score
df=pd.read_csv('German Credit Data.csv')
df_encoded=pd.get_dummies(df)
x=df_encoded.drop(columns=['status'])
y=df_encoded['status']
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=42)
classifier=RandomForestClassifier(random_state=42)
classifier.fit(x_train,y_train)
y_pred=classifier.predict(x_test)
conf_matrix=confusion_matrix(y_test,y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix,annot=True,cmap='Blues',fmt='d',xticklabels=['prediction bad','prediction good'])
plt.title('confusion matrix')
plt.xlabel('predicted label')
plt.ylabel('True label')
plt.show()
classification_labels=['Bad credit','Good Credit']
print('classification labels:')
for actual_class,row in zip(['Actual Bad','Actual Good'],conf_matrix):
    print(actual_class+':',','.join([f'{label}:{count}' for label,count in zip(classification_labels,row)]))
accuracy=accuracy_score(y_test,y_pred)
print('accuracy score:',accuracy)


EXP-3
•	Apply train test split and develop a regression model to predict the sold price of
•	players using ipl imb381ipl2013.csv dataset. Build a correlation matrix between all
•	the numeric features in the dataset and visualize the heatmap. Measure the RMSEof train and test data.
•	import pandas as pd
•	import numpy as np 
•	from sklearn.model_selection import train_test_split 
•	from sklearn.linear_model import LinearRegression 
•	from sklearn.metrics import mean_squared_error 
•	import seaborn as sns 
•	import matplotlib.pyplot as plt 
•	data = pd.read_csv("IPL IMB381IPL2013.csv")
•	print(data.head(2))
•	print(data.isnull().sum()) 
•	data.dtypes 
•	       data2 = data.drop(columns=['PLAYER NAME', 'COUNTRY', 'TEAM', 'PLAYING ROLE','AUCTION YEAR'])
•	y = data2['SOLD PRICE']
•	X = data2.drop(['SOLD PRICE'],axis=1)
•	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
•	model = LinearRegression()
•	model.fit(X_train, y_train)
•	pred_train = model.predict(X_train)
•	pred_test = model.predict(X_test) 
•	rmse_train = np.sqrt(mean_squared_error( pred_train,y_train))
•	rmse_test = np.sqrt(mean_squared_error( pred_test,y_test))
•	print("RMSE Train:", rmse_train)
•	print("RMSE Test:", rmse_test)
•	corr = data2.corr()
•	plt.figure(figsize=(40,32))
•	sns.heatmap(corr, annot=True, cmap="coolwarm")
•	plt.show() 
•	


EXP-=4
•	
•	For the glass identification dataset, fit random forest classifier to classify glass type.
•	Study the accuracy. Which among the so fitted models: KNN, naïve bayes, RandomForest is more suitable and why.
•	import pandas as pd
•	from sklearn.model_selection import train_test_split 
•	from sklearn.ensemble import RandomForestClassifier 
•	from sklearn.neighbors import KNeighborsClassifier 
•	from sklearn.naive_bayes import GaussianNB 
•	from sklearn.metrics import accuracy_score,classification_report,confusion_matrix 
•	import seaborn as sns 
•	glass_df=pd.read_csv("glass.csv")
•	X=glass_df.drop("Type",axis=1)
•	y=glass_df['Type']
•	X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
•	print(glass_df.head(2))
•	print(X.head(2))
•	print(y.head(2)) 
•	rf_model=RandomForestClassifier(random_state=42)
•	rf_model.fit(X_train,y_train)
•	rf_pred=rf_model.predict(X_test)
•	print("Classification report: ")
•	print(classification_report(y_test,rf_pred))
•	print("Confusion Matrix:")
•	print(confusion_matrix(y_test,rf_pred))
•	knn_model=KNeighborsClassifier()
•	knn_model.fit(X_train,y_train)
•	nb_model=GaussianNB()
•	nb_model.fit(X_train,y_train)
•	knn_pred=knn_model.predict(X_test)
•	nb_pred=nb_model.predict(X_test)
•	rf_accuracy=accuracy_score(y_test,rf_pred)
•	knn_accuracy=accuracy_score(y_test,knn_pred)
•	nb_accuracy=accuracy_score(y_test,nb_pred)
•	print("Random Forest Accuracy : ",rf_accuracy)
•	print("KNN Accuracy : ",knn_accuracy)
•	print("Naive Baysian :", nb_accuracy) 


EXP-5
•	
•	Apply hierarchical clustering and dendrograms to identify the optimal number of clusters on the online retail.csv dataset
•	import numpy as np 
•	import pandas as pd
•	import matplotlib.pyplot as plt 
•	from sklearn.cluster import AgglomerativeClustering 
•	dataset=pd.read_csv("OnlineRetail (1).csv",nrows=50)
print(dataset)
•	x=dataset[['Quantity','UnitPrice']].values
•	import scipy.cluster.hierarchy as hie 
•	import matplotlib.pyplot as plt 
•	dend=hie.dendrogram(hie.linkage(x,method="ward"))
•	plt.title("Dendrogram Plot")
•	plt.ylabel("Euclidean Distances")
•	plt.xlabel("Items")
•	plt.show()
•	hc=AgglomerativeClustering(n_clusters=5,affinity='euclidean',linkage='ward')
•	y_pred=hc.fit_predict(x)
•	plt.scatter(x[y_pred==0,0],x[y_pred==0,1],s=100,c='blue',label='Cluster1')
•	plt.scatter(x[y_pred==1,0],x[y_pred==1,1],s=50,c='red',label='Cluster2')
•	plt.scatter(x[y_pred==2,0],x[y_pred==2,1],s=200,c='green',label='Cluster3')
•	plt.scatter(x[y_pred==3,0],x[y_pred==3,1],s=10,c='cyan',label='Cluster4')
•	plt.scatter(x[y_pred==4,0],x[y_pred==4,1],s=10,c='magenta',label='Cluster5')
•	plt.title("clusters of items")
•	plt.ylabel("unitprice")
•	plt.xlabel("quantity")
•	plt.legend()
•	plt.show()


exp-6

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv("/content/Online Retail.csv", nrows=50000)

# Drop rows with missing values
df.dropna(inplace=True)

# Select relevant features for clustering
X = df[['Quantity', 'UnitPrice']]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Use the elbow method to find the optimal number of clusters
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plot the elbow curve
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

# Apply K-Means clustering with the optimal number of clusters
optimal_k = 3 # Adjust based on the elbow curve
kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Visualize the clusters
plt.scatter(X_scaled[clusters == 0, 0], X_scaled[clusters == 0, 1], s=100, c='red', label='Cluster 1')
plt.scatter(X_scaled[clusters == 1, 0], X_scaled[clusters == 1, 1], s=100, c='blue', label='Cluster 2')
plt.scatter(X_scaled[clusters == 2, 0], X_scaled[clusters == 2, 1], s=100, c='green', label='Cluster 3')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label='Centroids')
plt.title('Clusters of Customers')
plt.xlabel('Standardized Quantity')
plt.ylabel('Standardized Unit Price')
plt.legend()
plt.show()


exp-7

from sklearn import metrics
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
dataset=pd.read_csv('Mall_Customers.csv')
print(dataset)

dataset.rename(columns={'Annual Income (k$)':'Income','Spending Score (1-100)':'SpendScore'},inplace=True)
dataset=dataset.drop(['CustomerID'],axis=1)
dataset.head()
x=dataset.iloc[:,[2,3]].values
x.shape
from sklearn.cluster import DBSCAN
db=DBSCAN(eps=3,min_samples=4,metric='euclidean')
model=db.fit(x)
label=model.labels_
label


#identifying the points which makes up our core points
sample_cores=np.zeros_like(label,dtype=bool)

sample_cores[db.core_sample_indices_]=True

#Calculating the number of clusters

n_clusters=len(set(label))- (1 if -1 in label else 0)
print('No of clusters:',n_clusters)
y_means = db.fit_predict(x)
plt.figure(figsize=(7,5))
plt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], s = 50, c = 'pink')
plt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], s = 50, c = 'yellow')
plt.scatter(x[y_means == 2, 0], x[y_means == 2, 1], s = 50, c = 'cyan')
plt.scatter(x[y_means == 3, 0], x[y_means == 3, 1], s = 50, c = 'magenta')
plt.scatter(x[y_means == 4, 0], x[y_means == 4, 1], s = 50, c = 'orange')
plt.scatter(x[y_means == 5, 0], x[y_means == 5, 1], s = 50, c = 'blue')
plt.scatter(x[y_means == 6, 0], x[y_means == 6, 1], s = 50, c = 'red')
plt.scatter(x[y_means == 7, 0], x[y_means == 7, 1], s = 50, c = 'black')
plt.scatter(x[y_means == 8, 0], x[y_means == 8, 1], s = 50, c = 'violet')
plt.xlabel('Annual Income in (1k)')
plt.ylabel('Spending Score from 1-100')
plt.title('Clusters of data')
plt.show()
